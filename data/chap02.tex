
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

\chapter{相关工作}
\label{chap:background}

随着应用数量与规模的不断增加，数据中心所需要的服务器数量也在飞速增长中。
而受到电力供应、冷却系统以及占地面积等客观因素的影响，
服务器规模并不能够无限制的扩大，
服务器融合（Server Consolidation）成为当前应用部署的主流技术。
本节首先介绍当前常见的服务器融合技术；%，并对比其在资源、性能等方面的隔离性；
然后介绍在这种服务器融合的共享环境下，已有的服务质量与资源管理的相关工作；
最后对比网络领域中解决服务器质量与资源管理的方案，
讨论将其应用于计算机体系结构中的可行性。

\section{服务器融合技术}

通过考查现代数据中心的应用架构，可以发现其中大部分架构都符合``数据复用模型''，
即多个应用程序协同工作，每个应用只对数据进行部分处理，然后交给下级应用程序，
这些应用程序最终组合完成1个服务功能，提供给最终用户。
通过这种方式形成的复杂应用架构，由于在兼容性、性能或安全等多方面的原因，
各个模块通常不能很好的在同一台服务器内工作，而是需要多台服务器进行部署，
大量这样的多服务器应用给数据中心的维护工作带来了很大的挑战。
从另一方面，应用与服务器规模的不断增长，
数据中心在占地、电力消耗、冷却系统等方面也产生了极大的压力，
但服务器资源利用率过低的问题，造成数据中心整体的效率十分低下。
为解决这一问题，许多软硬件厂商都提出了自己的方法与技术，
将应用部署到更少的物理服务器中，实现高效可管理的服务器融合，
其中虚拟化、硬件分区与基于容器的轻量级虚拟化是3种目前应用最为广泛的服务器融合技术。

% 硬件分区
虚拟化技术最初由IBM在20世纪60年代提出，当时提出虚拟化的目的是为了提供系统的向后兼容性，
以简化用户编程，而后虚拟化一直是大型机基本的使用方式。
在此基础上IBM提出了逻辑分区（LPAR）\cite{IBM_LPAR:2007}技术，
该技术使得1台计算机能够像2台或更多有独立计算机一样运行，
为服务器融合提供了平台支持，
其他一些厂商如Hitachi\cite{hitachi-lpar}和Sun（Oracle）\cite{LDom}也提供了类似的解决方案。

% 软件虚拟化
VMware首先将虚拟化技术引入到基于x86的PC服务器领域，
由于当时x86架构并没有提供任何虚拟化的支持，
VMware使用二进制翻译\cite{vmware-compare-hw-sw:2006}
的方式实现操作系统内核中不支持虚拟化的指令执行，
如图\ref{fig:compare-of-virt}(a)所示，实现对用户操作系统透明的虚拟化方案。
这种基于二进制翻译的全虚拟化方案性能存在问题，
因此Xen提出了半虚拟化（para-virtualization）\cite{barham_xen_2003}的概念，
通过修改客户机操作系统，直接使用Hypercall的方式调用hypervisor
（如图\ref{fig:compare-of-virt}(b)所示），提高系统性能。
在虚拟化产业发展起来后，各个硬件厂商分别推出新的硬件功能以更好的支持虚拟化，
如Intel的VT-x技术和AMD的AMD-V技术，如图\ref{fig:compare-of-virt}(c)所示，
硬件辅助虚拟化逐渐成为主流。
随着虚拟化技术性能的不断提高，当前在PC服务器领域，
虚拟化技术已经成为数据中心内被普遍使用的技术。

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{bg/compare-of-virtualization}
  \caption[3种不同类型虚拟化技术的对比]{3种不同类型虚拟化技术的对比：
    （a）无虚拟化，直接执行用户与OS请求；
    （b）全虚拟化，直接执行用户请求，二进制翻译执行OS请求；
    （c）半虚拟化，直接执行用户请求，修改GuestOS通过Hypercall实现特权指令；
    （d）硬件辅助虚拟化，直接执行用户请求，硬件支持OS特权指令直接陷入到VMM。}
  \label{fig:compare-of-virt}
\end{figure}

%轻量级虚拟化
基于容器的轻量级虚拟化是另一种流行的服务器融合技术，
与虚拟机抽象类似，不同容器之间以及容器与主机操作系统之间是相互隔离的，
但它们共享一份操作系统内核，可以有效的提高资源利用率。
以Docker为例（如图\ref{fig:docker-overview}），
用户的应用与依赖的运行时环境被打包为一个容器，
容器之间使用Linux内核的LXC\cite{lxc}机制实现名字空间隔离，
同时使用Control Group\cite{cgroup}实现资源控制。
与虚拟化技术相比，容器技术最大的优点是它具有更少的资源占用，以及更快的启动时间，
因此也被普遍运行在数据中心场景中。

\begin{figure}[htb]
  \centering
  \begin{minipage}{0.75\textwidth}
    \includegraphics[width=0.75\textwidth]{bg/docker-overview}
    \caption[容器虚拟化结构示意（Docker）]{容器虚拟化结构示意（Docker）：
      容器中包含应用及其依赖的运行时环境，容器之间相互隔离但它们共享同一份操作系统内核。}
    \label{fig:docker-overview}
  \end{minipage}
\end{figure}

% TODO:各种服务器融合技术对比


\section{资源管理与服务质量保障}

%\textbf{参考Jigsaw/Ubik/VAntage的参考文献重写本节}

软硬件资源的竞争在整个系统栈中存在，
表\ref{tab:contention}列出了不同层次上可能存在的竞争点，
以及在不同层次上消除竞争点的相关工作。
Google和Twitter等互联网公司也尝试改进数据中心管理系统架构，
实现服务器资源利用率与应用服务质量之间的平衡，
一些典型系统包括：Borg\cite{borg:2015}、Omega\cite{Schwarzkopf_omega_2013}和
Mesos\cite{Hindman:2011:Mesos}。
另外一些研究工作尝试使用分布式技术或操作系统内核优化技术实现应用服务质量保障，
如多级优先级策略\cite{Reiss_googletrace_2012}、
调度方法\cite{delimitrou_paragon:_2013, delimitrou_quasar:_2014, mars_heterogeneity_2011,
kozyrakis_reconciling_2014, Novakovi:ATC2013}、
backup-request\cite{dean_tail_2013}，cgroup\cite{cgroup}和容器技术\cite{lxc}。 
本节主要关注节点内资源管理与服务质量保障的相关研究，包括软件和硬件两个方面。

\begin{table}[hb]
  \centering
  \caption{资源管理与服务质量保障相关工作中识别出的竞争点}
  \label{tab:contention}
  \begin{tabular}{p{0.15\textwidth}p{0.75\textwidth}}
    \toprule[1.5pt]
      {\heiti 层次}     & {\heiti 竞争点}                                                                                      \\
    \midrule[1pt]
      数据中心          & Global file system \cite{dean_tail_2013}                                                                        \\
      应用层            & Background daemon \cite{dean_tail_2013}, backup job \cite{yu_profiling_2011,dean_tail_2013}                     \\
      网络协议栈        & Nagle's algorithm, limited buffers, delayed ACK caused RTO \cite{yu_profiling_2011},
                          TCP congestion control \cite{alizadeh_data_2010,dong_less_2013},
                          packet scheduling \cite{vamanan_deadline-aware_2012,wilson_better_2011,zats_detail:_2012,hong_finishing_2012},
                          kernel sockets \cite{kozyrakis_reconciling_2014}                                                                \\
      操作系统内核      & Lock contention \cite{Kapoor:2012:Chronos}, context switch, kernel scheduling,
                          SMT load imbalance and IRQ imbalance \cite{kozyrakis_reconciling_2014}                                          \\
      虚拟化层          & Virtual machine scheduling \cite{Xu:2013:Bobtail:,wang_impact_2010, Xu:2013:SMALL},
                          network bandwidth \cite{wang_impact_2010,shieh_sharing_2011,Xu:2013:SMALL,jeyakumar_eyeq:_2013}                 \\
      硬件              & Shared caches \cite{kozyrakis_reconciling_2014,Tang:2011:ISCA,kasture_ubik:_2014,sanchez_vantage:_2011,sanchez_zcache:_2010,qureshi_utility-based_2006,DelimitrouK13:ibench},
                          memory \cite{Tang:2011:ISCA,yang_bubble-flux:_2013,muralidhara_reducing_2011,DelimitrouK13:ibench},
                          NIC \cite{Radhakrishnan:2014:SENIC},
                          I/O \cite{mesnier_differentiated_2011,DelimitrouK13:ibench}                                                     \\
    \bottomrule[1.5pt]
  \end{tabular}
\end{table}


\textbf{软件服务质量保障技术}\quad

在软件层次上主要采用隔离与调度2个方法实现服务质量保障。

%% 虚拟化可以提供隔离，但性能上很差，主要是硬件层次会有干扰，无法实现隔离
%虚拟化技术能够提供一定程度的
%
%

% 页着色可以通过软件实现cache和dram的划分，但不太灵活
页着色技术（page coloring）是一种以软件方式控制内存物理页映射的方法，
通常被用作共享末级缓存与DRAM的划分，其核心思路是通过控制地址映射信息实现硬件资源的管理。
一些研究利用该技术实现共享末级缓存划分\cite{lin_gaining_2008, tam_managing_2007}
来解决应用之间缓存竞争问题，
或在虚拟化场景下实现缓存划分\cite{Jin2009, Chen2010, Wang2012}；
还有一些研究\cite{liu_software_2012}使用该技术实现DRAM的bank划分，
来消除bank级的干扰与竞争，或实现DRAM与末级缓存的协作式划分\cite{Liu:2014:ISCA}。
但该技术存在2个方面的问题：
首先，当应用负载发生变化时，需要在内核中重新组织空闲页链表并进行必要的页面迁移，
这需要非常大的软件开销；
更为严重的是，目前的处理器通常使用复杂的哈希算法实现从物理地址到cache index的映射
（如Intel的SandyBridge架构），
而这些哈希算法通常并不会公开，因此在这样的系统上实现页着色是不现实的。

%从调度的角度也有一些研究，但都是ad-hoc的工作，无法普及
在软件调度方面也存在大量研究，
例如，CAER框架\cite{mars_contention_2010}是一种竞争感知的轻量级运行时环境，
能在提高利用率的同时减少由于片上或片外资源的竞争所引起跨核应用之间的干扰；
CiPE框架\cite{mars_directly_2011}可以直接测量和量化多核结构下应用的跨核干扰敏感度，
并以此为依据进行作业调度。
Jason Mars等人设计的Bubble-Up\cite{mars_bubble-up:_2011}机制，
通过使用气泡（Bubble）来代表内存子系统的可变压力情况，
能准确预测在内存子系统中竞争共享资源而导致的性能下降，通过离线分析来决定最优的应用混合，
Bubble-Flex\cite{yang_bubble-flux:_2013}工作在Bubble-Up离线的基础上，实现了在线的QoS管理。
ReQoS\cite{tang_reqos:_2013}提供了一种编译技术与运行时环境相结合的方法，
通过编译的方法标识出低优先级应用中可能引起竞争的代码段，
通过运行时环境调节低应用级应用的执行来确保这些代码段不会对高优先级应用的服务质量造成影响。
以上这些工作虽然能够缓解服务质量与资源利用率的冲突，
但它们的解决方案都只能针对特定的场景，不具有通用性。


\textbf{硬件服务质量保障技术}\quad

由于单纯使用软件技术无法解决应用在硬件层次的干扰，
目前学术界和工业界在硬件层次上提出一系列方案，解决干扰问题。
其中以处理器末级缓存与内存控制器上的硬件方案居多。

针对处理器末级缓存容量的竞争，Kasture和Sanchez提出的Ubik\cite{kasture_ubik:_2014}，
通过识别和利用延迟敏感型应用瞬时性的特点，调整缓存容量划分策略，来保证目标长尾延迟。
UCP\cite{qureshi_utility-based_2006}提出了一种应用缓存容量需求的探测方法，
为应用分配收益最大的缓存划分方案。
由于受到工艺、面积以及能耗的限制，目前的常见处理器末级缓存关联度最大只有24，
不足以分配给其上运行的应用，
针对该问题ZCache\cite{sanchez_zcache:_2010}提出了一种新的缓存方法，将缓存路与关联度解耦，
提高关联度为缓存容量划分提供更大的空间，
Vantage\cite{sanchez_vantage:_2011}基于该缓存设计实现了一种细粒度的缓存容量划分方案。

%Kasture and Sanchez propose Ubik \cite{kasture_ubik:_2014},
%a cache partitioning policy that characterizes and leverages the transient
%behavior of latency-critical applications to maintain their target tail latency.
%Vantage \cite{sanchez_vantage:_2011} implements fine-grained cache partitioning
%using the statistical properties of Zcaches \cite{sanchez_zcache:_2010}.
%Utility-based cache partitioning (UCP) \cite{qureshi_utility-based_2006}
%strictly partitions the shared cache depending on the benefit of allocating
%different number of ways to each application.

针对内存控制器上的竞争，
Muralidhara等人提出了一种应用感知的内存通道划分技术MCP\cite{muralidhara_reducing_2011}，
用于降低应用在内存子系统的干扰。
以CMU的Onur Mutlu为代表的一些研究提出一系列调度算法\cite{mutlu_stall-time_2007,
mutlu_parallelism-aware_2008, kim_atlas:_2010, kim_thread_2010}
以缓解内存控制器的不公平问题，从而提高系统吞吐量以及服务质量。
但这些算法是固化的，并不能针对某个应用进行调节，灵活性较差。

除硬件层次的划分与调度外，Andrew Herdrich等人\cite{herdrich_rate-based_2009}
发现了用于功耗管理的处理器限速技术（如DVFS），
能够应用到QoS领域用于实现处理器末级缓存与内存控制器上的竞争管理。
其基本方法是当正在运行的低优先级任务由于资源竞争争而干扰了高优先级任务的性能时，
就减缓核心的处理速率，通过请求抑制的方法缓解低优先级任务对共享资源的竞争。

在QoS保障框架方面，
Rafique等人提出了一种操作系统驱动的缓存容量划分机制\cite{Rafique:2006:ASO}，
通过标记Cache访问请求来允许操作系统根据标记对Cache容量划分进行管理。
Sharifi等人进一步提出了一种基于反馈的控制架构\cite{sharifi_mete:_2011}，
以实现端到端的片上资源管理。
Ravi Iyer提出了一种保障CMP体系结构上缓存Qos的管理框架CQoS\cite{iyer_cqos:_2004}，
将QoS工作分为3个阶段，即优先级分类（Priority Classification）、
优先级分配（Priority Assignment）和优先级实现（Priority Enforcement）。
然而，该工作只设计了QoS保障的框架，而并没有详细描述具体策略和软硬件支持。
后续的工作\cite{iyer_qos_2007, li_coqos:_2011, li_dynamic_2012}实现了
基于class-of-service的QoS架构（CoQoS），通过为片上请求标记优先级的方式，
实现基于优先级的Cache/DRAM/NoC请求调度。


\section{计算机网络与服务质量}
\label{sec:background:sdn}

服务质量问题在计算网络领域存在大量研究，从原型上来说主要分为3类。
第1类是资源过量分配，根据峰值需要分配网络资源，实现服务质量保障；
第2类是以集成服务（integrated services，IntServ）\cite{IntServ}和
区分化服务（differentiated services，DiffServ）\cite{DiffServ}
为代表的共享网络环境下的服务质量保障方法，
该方法的核心思路是在网络包中增加应用区分的标签，
可以是基于单个应用（IntServ）或基于服务类型（DiffServ），
以标签为粒度在网络链路上预留资源，实现服务质量保障。
第3类是以软件定义网络SDN\cite{SDN}为代表的可编程网络架构，
能够根据应用需求的变化，通过可编程的方式实现网络拓扑的调整，
同样能够实现服务质量保障。

当前数据中心区分在线应用与离线应用的方法，
映射到网络领域可以被看做是第1类和第2类方案的结合：
使用较粗的粒度对应用进行区分（在线、离线），
并为在线作业过量分配资源，以保障其服务质量。
从网络领域的经验来看，该方案会造成严重的资源浪费，
事实也是正是如此，数据中心只有6\%$\sim$12\%的资源利用率。
要解决数据中心资源利用率与服务质量冲突，可以从网络领域中借鉴更多的思路，
包括细粒度的应用区分、以及可编程的网络管理，
本文所提出的资源管理可编程体系结构正是基于以上思路。
通过在计算机体系结构内实现应用区分与可编程的资源管理，
来解决数据中心资源利用率与服务质量的冲突。

%\begin{figure}[tbh]
%  \centering
%  \includegraphics{arch/sdn-arch.pdf}
%  \caption{软件定义网络SDN架构}
%  \label{fig:pard-arch-outline}
%\end{figure}


\section{本章小结}

本章首先介绍了本文的研究背景与问题，
即新计算模式下数据中心面临应用服务质量与资源利用率相冲突的问题，
并从软件和硬件两个角度总结针对该问题的相关研究。
从现有技术来看，单节点内服务质量保障技术的不足，导致节点内应用相互干扰严重，
已经成为目前数据中心整体服务质量保障的短板，是成为长尾延迟现象的主要因素之一。
单纯从软件或硬件层次无法根本解决该问题，而是需要跨层次协同设计。
而软硬件协同设计的关键在于``应用如何表达服务质量（QoS）目标并且让底层的硬件、操作
系统以及虚拟层共同工作来保障它们''\cite{21st_architecture}。
本文后续章节将围绕该问题讨论如何设计一种新型的体系结构，
通过软硬件协同的方式解决数据中心当前面临的这一难题。

