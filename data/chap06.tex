
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

\chapter{资源管理软件栈研究}
\label{chap:prm}

%% 数据中心管理软件要做这些事情
%A datacenter management system brings the benefits of high availability, high
%reliability, high utilization and fast deployment and so on.
%% 它的基本构成是隔离与封装环境，当前使用虚拟化与容器实现
%An essential building block of such management systems is resource isolation and
%encapsulation, which now primarily adopts software based approaches such as
%virtualization and container.
%% 但软件方法没有足够的性能隔离
%However, software approaches are insufficient
%for performance isolation due to the interference on low-level hardware resources
%(e.g., shared cache, memory bandwidth).
%% 因此需要硬件支持
%Thus, to address this issue,
%recently hardware isolation has been proposed and proven to be promising.

数据中心对其管理系统提出了高可用、高可靠、高利用率以及快速部署等需求，
资源隔离与封装是实现以上需求的必要条件。
当前数据中心管理系统大都基于虚拟化、容器等软件技术实现资源隔离与封装。
然而受到底层硬件层次（如处理器末级缓存、内存控制器等）资源竞争与干扰的影响，
这些软件方法无法实现性能隔离，需要在硬件上提供资源隔离机制。

%% 软硬件是交替促进发展的，本章讨论硬件隔离机制如何影响软件的改变，
%% 具体来说本文所提出的PARD体系结构对上层管理软件提出的需求，
%% 包括Hypervisor、操作系统及上层管理软件。
%Since the hardware and the system software always evolve mutually, in this paper
%we discuss how hardware isolation influences system software, including
%hypervisor, operating system kernel and cluster management system.
%Specifically, we will outline how to modify OpenStack, a popular
%cloud computing platform, to be aware of servers with hardware isolation
%and further take the advantages of hardware isolation.

计算机的软硬需要协同工作，本章主要讨论硬件提供更多资源管理机制后，
对应的系统软件栈（e.g. hypervisor，操作系统，数据中心管理系统）需要如何适应这种变化。
具体来说，即如何在PARD体系结构下设计系统软件栈，以实现高效的数据中心管理。
本章内容安排如下：
首先以mesos为例，介绍数据中心管理系统
然后介绍基于PARD体系结构的数据中心架构，之后具体介绍单节点PARD软件栈构成；
并以mesos为例，讨论如何将PARD集成到数据中心管理架构中。

%实现高效资源管理需要软硬件协同设计。
%PARD体系结构已经实现将硬件资源信息暴露给上层软件，
%并提供可编程接口给软件访问，本章主要讨论在PARD这样的硬件架构平台上，
%如何设计软件栈以实现高效的资源管理。

%则一般都会影响甚至重构整个软件栈：最底层的控制平面驱动层，负责直接访问控制平面，这一层可以部署在操作系统、Hypervisor或者一个轻量级的操作系统内核；在驱动层上面为监控管理层，主要负责集中存储收集到的各个控制平面的统计数据，并将多个控制平面关联分析，进行资源分配管理的决策；更上层是用户编程接口层，主要负责提供对控制平面统一的抽象编程接口以及对统一、决策功能的抽象编程接口；最上层则是数据中心管理软件层，主要负责收集几个计算机节点的资源信息，进行全局作业调度与资源管理，以实现在保障关键应用服务质量的前提下达到全局资源利用率最优化。
%因此，从单个计算机节点角度出发，在硬件支持资源管理的基础上，软件栈的设计与实现需要研究上述驱动层、监控管理层与用户接口层的设计与实现，降低软件栈的设计复杂度与运行开销，研究针对不同资源的优化管理策略。同时研究在这种新体系结构下对软件虚拟化技术的影响。
%
%PARD体系结构在硬件上有两方面的改变：一是一部分资源管理功能放到了硬件控制器上；二是将更多的硬件资源信息暴露给软件。计算机底层硬件功能的改变会对上层软件栈产生影响。

\section{背景}

管理大规模数据中心是大型互联网公司运维工作的基本任务，在过去的十几年中，
这些公司开发了不同的管理系统来完成这一任务，
如：Google的Omega\cite{Schwarzkopf_omega_2013}、Borg\cite{borg:2015}、
Kubernetes\cite{Kubernetes}，Microsoft的Apollo \cite{Apollo}、Cosmos \cite{Cosmos}，
Facebook的Aurora\cite{Aurora}，国内百度的Matrix以及阿里的伏羲\cite{Fuxi}。
其他一些开源系统如OpenStack\cite{OpenStack}、Apache Mesos\cite{Hindman:2011:Mesos}、
YARN\cite{YARN}被广泛的应用在工业界。

除提供高可用、高可靠、快速部署等基本管理功能外，
这些数据中心管理系统的另一个重要目标是提高数据中心的资源利用率：
通过应用调度将数据中心百万量级的应用混合运行在十万量级到服务器上，
使用资源共享的方式充分利用服务器资源。
通常情况下，一台物理服务器上会运行多个具有不同资源与服务质量需求的应用。

%However, such workload colocation causes contention over shared hardware and software
%resources such as last level cache (LLC), network bandwidth and kernel buffers, thereby
%resulting in unpredictable performance variation that degrades QoS of
%latency-critical workloads.
%In order to guarantee these workloads' QoS in shared environments,
%current management systems primarily adopt software based resource
%isolation mechanisms, e.g., virtual machine, container and
%control group (cgroup) \cite{cgroup}, due to the lack of hardware supported isolation.
%
%Software based isolation is effective to a certain extend for resource allocation
%and protection but is insufficient for performance isolation, especially due to
%contention over hardware resources.
%
%For example, with more than 10 years of
%experience in operating Borg in Google's production environments, Borg developers
%found that after using a Linux cgroup-based resource container, low-level
%hardware interference still happens \cite{borg:2015}.
%Thus, in Dick Sites' recent talk \cite{}, he argued that better hardware
%support isolation is needed.
正如本文前几章所分析的，多个应用在共享软硬件资源上竞争会对应用造成不可预测的性能下降，
特别是延迟敏感型应用，这种性能下降更为明显。
为了保障在共享环境下关键应用的服务质量，
当前的数据中心管理系统通常会使用软件方法来实现应用之间的隔离，
如虚拟化、容器和cgroup。
虽然软件隔离方案能够很好的实现资源分配与保护，
但它不足以提供有效的性能隔离，特别是由硬件层次上资源竞争所造成的性能干扰。
以Google数据中心为例，即使经过10多年生产环境的运行与优化，
基于cgroup实现软件隔离的Borg系统仍然无法解决硬件层次所带来的干扰\cite{borg:2015}。
因此，正如Dick Sites在其报告\cite{Dick:2015}中所提出，
硬件支持隔离对于数据中心服务器是必不可少的。

本文所提出的资源管理可编程体系结构（PARD）正是这样一种支持硬件隔离的服务器体系结构，
它能够实现全硬件支持虚拟化（第\ref{chap:labeladdrspace}章），
在无需软件Hypervisor的支持下，即可将一台物理服务器划分为多个相互隔离的子机器（逻辑域），
在逻辑域内可以像虚拟机一样运行未修改的操作系统与应用。
同时PARD还提供了以逻辑域为粒度的区分化服务，
通过可编程的硬件资源管理机制实现逻辑域之间的资源划分与性能隔离
（第\ref{chap:hwresman}章）。
基于模拟器的实验结果表明，PARD能够在硬件上实现有效的资源与性能隔离，
平衡应用服务质量与服务器资源利用率。

%To address this issue, we recently proposed a new computer architecture PARD
%(Programmable Architecture for Resourcing-on-Demand) \cite{Ma:2015:PARD}, which
%supports hardware isolation. Specifically, a PARD server is able to provide fully hardware supported
%virtualization without software hypervisors. Thus the server is
%physically partitioned into multiple submachines that can run unmodified OS.
%The PARD server also supports differentiated service (DiffServ) for these submachines,
%which means that each submachine is assigned a priority for performance isolation
%in light of its QoS requirement. Preliminary results on
%a GEM5 \cite{binkert_gem5_2011} based full-system
%cycle accurate simulator and an FPGA-based ongoing PARD server prototype
%further show that hardware isolation is feasible.

%Historically, the hardware and the system software always evolve mutually, such as
%interrupt and time-sharing, memory management unit (MMU) and virtual memory.
%Since recent efforts suggests that the trend of hardware isolation is promising,
%consequent questions arise: \textbf{what is the impact of hardware isolation on system
%software? What's next for system software?}

从计算机发展的历史来看，硬件与系统软件总是总是在相互促进发展的，
如硬件中断机制的出现使得系统软件发展出分时系统、MMU硬件的出现促成了虚拟内存机制。
本文所提出的PARD体系结构，以及其他一些硬件隔离技术的出现\cite{intel-rdt}，
势必也会对未来系统软件的设计产生影响。

本章主要讨论如何将PARD服务器集成到现有数据中心管理系统，
并利用其提供的资源管理特性（表\ref{tab:pard-features}）实现高效的数据中心管理。
PARD服务器与传统服务器主要的区别在于其提供了细粒度的资源管理、实时的性能监控、
硬件支持的虚拟化与性能隔离，因此更适合于短时多变的作业；
而长期运行，且负载稳定的应用可以通过软件管理的方式在传统服务器中运行。
因此，传统服务器与PARD服务器将混合共存于数据中心，由数据中心管理系统进行统一管理，
为不同类型的应用提供服务。
为实现以上架构，需要考虑两个问题：
1）如何设计节点内资源管理，使其充分发挥PARD体系结构提供的硬件支持；
2）如何修改数据中心管理系统，使其能够支持PARD体系结构。
为解决以上2个问题，本节后续内容将对数据中心管理系统与节点内资源管理架构进行简要介绍。

\begin{table}[hb]
  \centering
  \begin{minipage}[t]{0.7\linewidth}
  \caption{PARD服务器与传统服务器特性对比}
  \label{tab:pard-features}
  \begin{tabular}{p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}}
    \toprule[1.5pt]
                             & \textbf{传统服务器} & \textbf{PARD服务器} \\
    \midrule[1pt]
      \emph{Virtualization}  & SW Supported        & HW Supported        \\
      \emph{Perf. Isolation} & Unsupported         & HW Surpported       \\
      \emph{Monitoring}      & High overhead       & Realtime            \\
      \emph{Perf. Adaption}  & Coarse-grained      & Fine-grained        \\
    \bottomrule[1.5pt]
  \end{tabular}
  \end{minipage}
\end{table}


\subsection{数据中心管理系统}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{swstk/mesos-arch}
  \caption{Mesos系统架构}
  \label{fig:mesos-arch}
\end{figure}

如上文所述，数据中心管理系统需要为数据中心提供高可用、高可靠、高利用率以及快速部署支持。
以Apache Mesos\cite{Hindman:2011:Mesos}为例，其架构如图\ref{fig:mesos-arch}所示，
它通过集中式的master节点控制运行在服务器内的slave守护进程实现数据中心资源管理。
master负责管理所有的资源，当slave注册到mesos集群时，将其提供的资源发送到master进行统一管理，
这些资源主要指CPU、内存、磁盘，除此之外还包括其他用户自定义类型的资源，
以资源列表的形式发送给master，如：

\textit{<slave ID, resource1:amount1, resource2:amount2, ...>}

mesos为用户提供framework抽象，主要包含两个部分：调度器（scheduler）与执行器（executor）。
其中调度器与master节点交互并获取资源，执行器在slave节点上运行，执行由调度器发送的任务（task）。
master根据用户设定的策略为每个framework分配资源，例如公平划分（fair sharing）或基于优先级的策略（strict priority）。
当master决定分配多少资源给framework后，将资源列表发送到framework的调度器，
调度器从中选择所需的资源，并将调度执行的任务描述发送给master，由其分发到对应的slave节点执行。

%\textbf{mesos如何实现高可用、高可靠、高利用率以及快速部署？}
mesos只提供了最基本的资源管理与调度功能，
通过跨节点的细粒度资源管理与调度，实现了高利用率；
并提供基于进程或容器的快速应用部署。
通过自定义framework可以很容易的扩展mesos，
如Marathon\cite{}在mesos的基础上实现了应用自动扩容与出错重启，实现了高可用与高可靠。


\subsection{节点内资源管理架构}

数据中心管理系统所提供的功能需要节点内提供相应支持，包括资源分配与隔离、资源监控等。
节点内资源管理可以分为带内管理（in-band management）与带外管理（out-of-band management）两类。
带内管理是指管理系统需要与应用系统运行在同一个平台上，
将管理系统安装在服务器上，以实现对系统资源的管理，
如Hypervisor以及上节介绍的Mesos都属于此种类型；
带外管理使用独立的模块实现系统管理，典型的系统如IPMI/BMC。

PARD体系结构采用带外管理的方式管理节点内的资源，其PRM的设计是源于服务器中IPMI/BMC的设计，并对其功能进行扩展。
原始的IPMI/BMC只提供了XXX的功能，其架构如图\ref{fig:ipmi-schem}所示，

The Intelligent Platform Management Interface (IPMI) is a set of computer interface specifications for an autonomous computer subsystem that provides management and monitoring capabilities independently of the host system's CPU, firmware (BIOS or UEFI) and operating system. IPMI defines a set of interfaces used by system administrators for out-of-band management of computer systems and monitoring of their operation. For example, IPMI provides a way to manage a computer that may be powered off or otherwise unresponsive by using a network connection to the hardware rather than to an operating system or login shell

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.95\textwidth]{swstk/ipmi-schem}
  \caption{IPMI结构框图}
  \label{fig:ipmi-schem}
\end{figure}

IPMI的结构如图\ref{fig:ipmi-schem}所示，其中主要包含以下几个模块：

\textbf{Baseboard Management Controller (BMC)}\quad
A micro-controller (BMC) is the heart of the IPMI architecture. The tasks of the BMC includes:
interfacing between the system management software and the hardware being used (through which the BMC has been connected using IPMB and ICMB)
monitoring independently
logging events independently
controlling recovery

\textbf{Intelligent Platform Management Bus (IPMB)}\quad
IPMI allows for the extension of the BMC by additional Management Controllers (MCs) through the application of the IPMB standard.
IPMB is an I²C based serial bus, which makes connection with various boards inside of one chassis possible. It is used for communication to and between the management controllers (MCs). Additional MCs are often designated Satellite Controllers.

\textbf{Intelligent Chassis Management Bus (ICMB)}\quad
ICMB provides a standardized interface for communication and control between chasses.

PRM在其基础上通过控制平面与控制平面网络，将资源管理功能也增加到其中，PRM具体架构在第\ref{chap:prm:arch}节将进行详细介绍。


\section{软件栈平台与架构}
\label{chap:prm:arch}

PARD使用带外管理的方式实现硬件资源的管理，
与资源管理相关的软件栈运行在独立的平台资源管理模块（PRM）上。
如图\ref{fig:pard-swstk-arch}所示，
PRM是一个嵌入式的SoC系统，其中集成了处理器、内存、flash存储、以太网接口等模块；
与IPMI/BMC模块使用IPMB/ICMB总线连接其控制单元类似，
PRM通过控制平面网络（Control Plane Network，CPN）连接系统中所有的控制平面，
以实现节点内集中式的资源管理；
多台服务器的PRM模块共同连接到数据中心管理系统中，实现数据中心内统一资源管理。

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\textwidth]{swstk/pard-swstk-arch}
  \caption{PARD软件栈架构示意图}
  \label{fig:pard-swstk-arch}
\end{figure}


PRM上运行基于Linux操作系统的固件，使用分层的方式实现，如图\ref{fig:prm-swstk}所示，
从下到上依次是：1）控制平面驱动；2）控制平面抽象层；3）应用适配层。

控制平面驱动层提供对所有控制平面的访问功能，
它会在/dev目录下为每一个连接到CPN上的控制平面创建对应的设备文件
（\textit{e.g.} /dev/cpa[0-9][0-9]*），
对这些设备文件的访问会被驱动转换为对16位的控制平面地址空间的访问，
通过第\ref{chap:hwresman}章所介绍的控制平面访问接口，
实现对控制表及数据平面的访问。

在控制平面驱动层的基础上，控制平面抽象层提供了更加友好的控制平面访问接口。
利用linux的sysfs\cite{patrick_mochel_sysfs_2005}机制，实现控制平面的抽象，
在/sys目录下为每个控制平面建立如图\ref{fig:pard-swstk-arch}所示的目录结构
（\textit{e.g.} /sys/cpa/cpa[0-9][0-9]*），提供对控制平面的统一编程接口。
用户可以使用文件系统系统调用（如read/write/ioctl），
操作/dev目录下的设备文件实现控制平面编程，
也可以直接使用bash命令（如echo/cat等）操作/sys目录下的文件实现对控制平面编程。
直接操作/sys中的文件是更为推荐的方式，下节将详细描述控制平面抽象以及/sys文件系统的功能。

应用适配层提供PARD体系结构在不同使用场景中的接口，
本文PARD原型系统实现了本地逻辑域和Mesos Daemon两个接口，
分别用于单节点资源管理与数据中心多节点资源管理。

当服务器加电后PRM首先启动，由控制平面驱动枚举控制平面网络，识别系统中的控制平面；
然后通过控制平面获取服务器的硬件资源信息，并建立sysfs下的控制平面抽象；
后续管理员或数据中心管理系统通过应用适配层与PRM交互，
将PARD服务器配置为多个逻辑域，运行用户的应用。

%管理员可以为每个应用设定其监控条件，当某一硬件控制平面发现监控条件满足时，
%则通过PRM通知上层管理架构，管理员可以通过PRM对不同应用的硬件资源使用进行管理。
%本节后续将从逻辑域的创建、监控条件设定、参数调整三个方面介绍PARD服务器操作。

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{swstk/prm-swstk.pdf}
  \caption[PRM软件栈示意图]{PRM软件栈示意图}
  \label{fig:prm-swstk}
\end{figure}


\subsection{控制平面抽象}
\label{chap:prm:arch:cpabs}

PRM利用linux的sysfs机制为用户（数据中心管理系统）提供控制平面的统一访问接口，
如图\ref{fig:prm-swstk}所示。

在/sys/cpa目录下，每个控制平面被表示为一个树型目录结构，
在每个目录下都会包含一些该控制平面的基本信息，
如控制平面标识ident、控制平面类型type，
本文实现的PARD原型系统支持的控制平面类型包括：
Cache（``C''）、内存控制器（``M''）和I/O Bridge（``B''）。
在控制平面的目录下还包含ldoms目录，
其中包含两个用于逻辑域控制的文件``do\_create''和``do\_destroy''，
分别用于创建和销毁逻辑域；
ldoms目录下的每个子目录代表该控制平面上当前存在的逻辑域。
向do\_create文件写入16位逻辑域编号即可在当前控制平面中创建该逻辑域，
例如使用如下命令即可在cpa0控制平面下创建逻辑域LDom\#4：
\begin{verse}
\textit{echo} 4 \textit{> /sys/cpa/cpa}0\textit{/ldoms/do\_create}
\end{verse}
与之相对，向do\_destroy文件写入16位逻辑域编号则会销毁当前控制平面中指定的逻辑域，
使用如下命令，即可将刚刚创建的逻辑域LDom\#4销毁：
\begin{verse}
\textit{echo} 4 \textit{> /sys/cpa/cpa}0\textit{/ldoms/do\_destroy}
\end{verse}

每当逻辑域被创建后，
其所在控制平面的ldoms目录下都会创建一个与逻辑域同名的子目录
（\textit{e.g.} ldom[0-9][0-9]*），
例如上文创建逻辑域LDom\#4的命令同时会在``/sys/cpa/cpa0/ldoms''
目录下创建名为ldom4逻辑域目录。
该逻辑域目录下包含parameters、statistics和triggers 3个子目录，
分别对应于控制平面内的3个控制表。
其中parameters和statistics子目录下的文件与其控制表表项一一对应，
以图\ref{fig:prm-swstk}中Cacge控制平面cpa0为例，
其参数表的参数way\_mask、以及状态表的miss\_rate和capacity都有相应的sysfs文件。
triggers目录与另外2个目录略有不同，该目录下为每一个有效的触发表项建立一个文件，
通过这个文件能够设置其触发条件满足时所执行的动作，
例如命令：
\begin{verse}
\textit{echo /scripts/some-scripts.sh > /sys/cpa/cpa}0
\textit{/ldoms/ldom}0
\textit{/triggers/}0
\end{verse}
会设置0号触发条件所执行的动作。
在实际情况下，不仅可以使用shell脚本作为触发动作，
在PRM上支持的任意语言的脚本、甚至可执行的ELF文件都可以作为触发动作，
通过sysfs安装到对应的触发条件上。
当前的原型系统实现，无法使用sysfs接口创建新的触发条件，
只能访问/dev目录下的设备文件，直接操作触发表表（ttab）来创建触发条件，
sysfs接口只提供对触发动作的修改功能。


\section{节点内资源管理}

数据中心管理系统所提供的功能需要节点内提供相应支持，
本节介绍PRM软件栈如何实现这些支持，具体包括
逻辑域管理、资源隔离和性能监控三个方面。

\subsection{逻辑域管理}

PARD服务器使用逻辑域抽象为用户提供服务，每个逻辑域都是PARD服务器硬件资源的子集，
可直接在其中运行操作系统。
不同的逻辑逻辑域独占部分资源，如处理器核、内存、I/O设备；
另一些硬件资源在不同逻辑域中共享，如L2 Cache。
当PRM收到逻辑域的创建请求后，PRM首先确认当前服务器是否有足够的资源满足逻辑域的需求，
如果满足，则为其分配一个本地唯一的逻辑域标识，
之后在各个硬件资源的控制平面中为该逻辑域分配所需的资源。
例如，用户创建的逻辑域需要2个处理器核、2GB内存、一个硬盘和一个网卡,
PRM在确认本地资源充足后，为其分配逻辑域标识LDom\#1，后续的创建流程如下：

\begin{enumerate}[leftmargin=2\parindent, nolistsep, label=\arabic*）]
  \item 查询处理器核的控制平面，找到两个空闲的处理器核，并向其标签寄存器写入LDom\#1，
        这两个处理器核后续发出的请求中将都包含该逻辑域标签。
  \item 在L2 Cache控制平面中增加LDom\#1的控制表项，由于该用户在创建逻辑域时并没有对
        Cache指定特殊需求，因此为其分配默认的替换策略掩码，与其他用户共享L2 Cache。
  \item PRM查询内存地址分配，找到空闲的2GB内存空间，
        在内存控制器控制平面中建立逻辑域LDom\#1的控制表项，
        将查询到的2GB空闲内存的地址写入控制表项，保存逻辑域与内存地址的关联，
        内存控制器在收到后续带有LDom\#1标签的访存请求后，会将查询地址映射信息，
        将来自不同逻辑域的访存地址映射到其对应的内存区域中。
  \item PRM为逻辑域分配磁盘和网卡，通过I/O控制平面将两个设备的标签寄存器设置为LDom\#1。
        这两个设备对内存的DMA请求中都包含该标签，保证其能够访问到逻辑域的内存地址空间；
        当前大部分的PCI-E设备都使用MSI方法产生中断，而MSI的本质是对特殊地址的访存请求，
        该请求中已经包含了逻辑域标签，
        因此中断请求能够被正确的发送到分配给该逻辑域的处理器核。
  \item PRM查询这两个设备在I/O总线上的物理地址以及所需的I/O地址空间长度，
        在逻辑域的地址空间中为这两个设备分配地址空间，
        通过I/O控制平面将分配的逻辑域地址空间与设备的物理地址映射记录到LDom\#1的控制表项。
\end{enumerate}

在资源分配完成后，PRM通过处理器控制平面，将两个处理器核复位，
之后选择一个处理器核作为Bootstrap Processor（BSP），令其开始执行BIOS代码，并启动操作系统。
由于不同的逻辑域使用不同的标识，在整个计算机的数据通路中都能够根据该标识对逻辑域实现区分，
实现逻辑域之间的资源隔离。

用户的应用在运行时对硬件资源的需求会发生变化，PARD提供了运行时调整硬件资源分配的方法，
允许用户根据需求在运行时对逻辑域的资源分配进行调整，以满足应用的需求。
硬件资源包括可透明调整的资源，如Cache容量、访存带宽等；
也包括需要操作系统支持的资源，如处理器核、内存容量、I/O设备。

在创建逻辑域时，同时还可以指定其性能参数，如Cache替换策略掩码、访存调度优先级、
I/O带宽等，各个硬件部分根据其控制平面中指定的性能参数实现对不同逻辑域的区分化服务。


\subsection{资源隔离}

软件怎么实现资源隔离：cgroup、hypervisor

PARD以逻辑域为粒度进行资源隔离。



使用默认参数创建的逻辑域，其资源是完全共享的。

PRM提供接口实现对不同逻辑域的资源进行隔离，具体方法如下，以Cache为例。




\subsection{性能监控与反馈}

按照性能监控的发起者，可将性能监控分为pull和push两类。
目前大多数系统都使用pull的方式获取硬件资源监控信息，
如处理器内常见的Performance Counter，通过xxx，获取监控信息。
这种方式需要用户主动参与，才能获取信息。
PARD在控制平面上使用硬件实现资源监控，以硬件请求为粒度，将实时的状态信息更新到其中状态表中，
通过PRM软件栈的sysfs抽象，可以随时获取实时的状态信息。

性能监控与反馈主要得益于控制平面的设计，通过在硬件上增加控制平面，对请求进行处理，
可以获得不同的应用的状态。如Cache缺失率、访存延迟等。
并通过一个可编程的触发逻辑，当特定事件发生后，将消息通过统一的控制平面网络，
发送到集中式的资源管理模块PRM，由PRM对资源分配进行调整。

Programming Methodology
As mentioned in x3, we adopt a ``trigger$\Rightarrow$action'' programming
methodology. Data center operators predefine a set of trigger)action
rules for different priorities. Usually a trigger is based on performance
metrics, e.g., ``MissRate>30\%''.
Example 1 in Figure 6 demonstrates how to install a trigger)action
rule. A data center operator uses the pardtrigger command to program
the trigger condition ``MissRate>30\%'' for ``ldom=0'' (i.e.,
DS-id=0) into the trigger table of the cache control plane (cpa0).
The parameter ``action=0'' guides the command to create a leaf
node ``0'' under ``.../cpa0/ldoms/ldom0/triggers''. Then the operator
calls ``echo ...'' to install the action script ``/cpa0-ldom0-t0.sh''
shown in Example 2.
There are two programming approaches based on the device file
tree. One is invoking system calls (open/read/write etc.) to open
and manipulate a CPA file. For example, the pardtrigger command
is written in C and invokes syscalls to install a trigger into the
cache control plane. The advantage of this approach is very fast. A
more convenient approach is leveraging bash commands to directly
access these CPA files, as shown in Example 2.


\section{与数据中心结合}

% 云计算三种模型
云计算有三种模式：SaaS、PaaS、IaaS，如图\ref{fig:cloud-usage-model}所示，
其区别在于用户需要管理的层次
其中IaaS用户需要管理到OS层，以OpenStack为例；
PaaS用户只需要管理应用与数据，其他部分由数据中心管理，以Docker为例；
SaaS是最底层，用户直接使用不同软件组合，以XX为例。
本章主要考查IaaS与PaaS两种模式。

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\textwidth]{swstk/cloud-usage-model}
  \caption{云计算3种使用模式对比：IaaS、PaaS与SaaS}
  \label{fig:cloud-usage-model}
\end{figure}

PARD提供的逻辑域抽象，能够很容易的实现IaaS模式，
因为逻辑域抽象与虚拟机抽象几乎完全类似，之前几章的工作都是基于IaaS模式对PARD进行讨论，
但目前数据中心PaaS由于资源浪费较少，被越来越广泛的使用，
本节主要讨论如何在PARD平台上实现PaaS模式，同时将其集成到数据中心管理系统中，
这里将主要针对mesos系统进行讨论。

\subsection{使用PARD体系结构实现PaaS}
% PARD如何实现PaaS
PARD实现PaaS的方式是通过类似于unikernel/LibOS的方式，
用户提供应用，PARD软件栈提供OS kernel，并将两者组合运行在逻辑域中。
具体流程如下，如图\ref{fig:pard-ldom-paas}：

1）用户提供需要执行的应用与数据，并选择所需的运行时环境和库；

2）将应用、数据、运行时环境打包为一个package，完成应用构建阶段；

3）系统根据用户选择的硬件资源配置创建逻辑域，将选择的内核与打包好的应用一起加载到逻辑域中；

4）将逻辑域在PARD服务器上启动。

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{swstk/pard-ldom-paas}
  \caption{PARD体系结构实现PaaS模式}
  \label{fig:pard-ldom-paas}
\end{figure}

通过以上方式实现硬件支持的容器，该方法也可以在其它只提供IaaS抽象的场景中实现PaaS的功能。

\subsection{mesos系统集成}
% mesos如何管理资源

% PARD如何支持MesosExecutor


\subsection{PARD体系结构对数据中心管理系统的影响}
% 对mesos资源模型的影响
%  -- PARD为mesos增加了额外可控制的资源
%  -- mesos如何管理这些额外的资源
%    -- 增加资源抽象属性：CPU/Mem => Cache/BW/QoS-Level/etc.
%    -- 增加调度考查

\section{小结}


应用负载具有波动性，对硬件资源的需求会发生变化，需要提供一种动态调整应用资源分配的方案。

有三个层次，分别：
(1)节点内不同硬件资源的协同管理；
(2)节点内应用调度与资源协同管理；
(3)节点间协同管理；


PRM软件接口，

- 与mesos集成，实现硬件支持的容器

- 与OpenStack集成，实现IaaS平台

- 与SDN集成，实现网络中心的系统

