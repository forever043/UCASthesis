
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

\chapter{标签化地址空间}
\label{chap:labeladdrspace}

以虚拟化技术为代表的多租户模式成为数据中心的主流应用模式。
在这种场景下，为不同用户提供良好的隔离环境是十分重要的。
现有大量技术的提出都是为了解决虚拟机之间隔离性，如：
VPID、
VMX扩展、
扩展页表EPT、
虚拟化的I/O APIC、
PCI-E单根虚拟化SR-IOV、
缓存容量划分CAT等。
隔离分为功能隔离与性能隔离，对于功能隔离，现有技术已经很好了，
能够达到虚拟机之间不存在相互干扰；
但在性能隔离方面，片内与片外存在明显差别。
以处理器核为例，
早期的软件虚拟化技术，需要软件实现处理器核状态保存与切换，还需要处理TLB和Cache无效，
Intel/AMD都在硬件上提供了处理器核虚拟化功能，如Intel的VMX和VPID技术，
在硬件上提供了处理器核状态切换，同时通过VPID技术使得切换虚拟机时无需进行TLB与Cache的无效操作；
早期guest的缺页处理需要由hypervisor拦截并处理，EPT技术增加了一级页表，将不同的虚拟机隔离开；
降低guest在hypervisor处理缺页时的干扰；
CMT技术允许给处理器核发出的访存请求打标签，另其在共享缓存层次实现应用区分，
按应用为粒度统计缓存占用、缺失率等信息；CAT在CMT的基础上实现缓存容量划分。

%
%在片外，I/O只划分不隔离，因为它离应用太远，已经没有应用的信息，
%
%Cache能做到很好的隔离是因为处理器核将标签传递到缓存
%
%但是该标签到Cache断了，
%
%所以提出标签化地址空间，将应用标签传递到整个系统，让所有的共享硬件都能够支持隔离
%标签化地址空间的目的是将多个虚拟机真正的物理隔离开。
%

标签化地址空间是PARD体系结构的基础，通过将应用标签传递到体系结构内所有的硬件部件中，
可以让这些共享的硬件部件能够识别出不同的应用，实现应用的资源隔离与性能隔离。
通过标签化地址空间以及少量的硬件修改，PARD可以很容易的将一台计算机划分为多个相互隔离的逻辑域，
在无需虚拟化软件（如KVM、Xen、VMware等）的辅助下，即可直接将这些逻辑域作为虚拟机使用；
同时，硬件能够通过标签获得更多来自上层应用的信息，辅助其调整自身的策略，以更好的服务应用。

%很多现有的技术与PARD的标签化地址空间具有想类似的理念，如EPT、SR-IOV、CAT/CMT等，
%它们都是希望在硬件上将不同的应用隔离开来，同时传递更多的应用信息到硬件。
%本章首先对标签化地址空间及其产生背景进行介绍，并与其它类似的技术进行对比，
%之后分析实现一个支持标签化地址空间的体系结构需要考虑的主要问题，
%最后讨论如何在现有体系结构实现下扩展以支持标签化地址空间。

%本章提出了一种标签化地址空间，通过将应用标签传递到整个系统，实现系统级按应用隔离，
%用以支持多租户场景下用户之间的隔离。
本章首先介绍相关工作，包括需要隔离的硬件部件，讨论在这些部件上已有的隔离技术，并分析其不足;
之后依次讨论如何在各个硬件部件上通过标签实现隔离；
最后讨论如何将标签化地址空间扩展到整个数据中心范围。


%%%虚拟地址空间对应多进程场景，随着单节点计算能力的增强以及云计算场景发展，
%%%多租户场景成为一种趋势。
%%%
%%%虚拟机抽象的出现是应对这下趋势的一个变化，将一台物理机隔离为多个无关的虚拟机，
%%%现在虚拟机基于的硬件技术包括：内存划分技术EPT、I/O虚拟化I/O-MMU、MSI中断等。
%%%这些技术在Hypervisor的支持下协同工作，向上层展现出虚拟的隔离计算机。
%%%
%%%但这里的隔离并非真正的隔离，多个虚拟机仍然通Hypervisor实现对共享硬件资源的使用，
%%%如hypervisor需要对页表寄存器进行控制，实现地址空间的切换；
%%%通过对I/O设备的代理访问，实现虚拟I/O设备；
%%%
%%%
%%%
%%%为了适应当前多应用以及多租户的使用场景，计算机系统软件栈在不断发展。
%%%以Linux操作系统为例，其包含多级嵌套结构以适应于不同的应用场景，如图\ref{}所示。
%%%操作系统最小的调度单元是线程，多个线程运行在同一个进程的地址空间中，
%%%线程之间共享全部的软硬件资源；
%%%不同的进程具有独立的虚拟地址空间，它们之间共享除用户地址空间外的全部软硬件资源，
%%%如操作系统内核；
%%%进程包含在名字空间中，一个操作系统中可同时存在多个名字空间，
%%%它们拥有自己的PID、网络、根文件系统，名字空间只共享部分操作系统内核，硬件依然是全共享；
%%%在Linux操作系统的最顶层是虚拟机，每个虚拟机都运行自己的操作系统，
%%%在操作系统及以上层次不存在共享，虚拟机之间通过Hypervisor共享硬件资源。


\section{相关工作}

体系结构对应用区分的支持，从最早的虚拟地址空间，到虚拟化支持。
本节将从资源隔离与性能隔离两个方面介绍应用区分的相关工作。


\subsection{总线技术}

\subsubsection*{QPI/HT总线}

\subsubsection*{PCI-E总线}

\subsubsection*{AXI总线}

\subsection{I/O中断}
% LegacyIntr => MSI

\subsection{资源隔离技术}
% VPID, EPT, I/O MMU, Virtualized I/O-APIC, SR-IOV


最早被隔离的资源是CPU，通过保存寄存器状态，并在进程切换时切换寄存器，实现CPU资源共享。
随着内存需求的增加，虚拟内存出现，实现用户地址空间的隔离。
虚拟化的出现，对虚拟机访存性能提出需求，体系结构也做出了相应的更改。
以Intel为例，为更好的支持虚拟化环境，其VMX技术增加了VCPU抽象，

单一地址空间 => 虚拟内存（多地址空间，保护）=> 虚拟化，tagged-TLB，VT技术，多租户
                                            => 容器技术
对计算机的使用趋势是从单一应用到多应用，

\subsection{性能隔离技术}
% CAT/CMT

\subsubsection*{Intel Resource Director Technology（RDT）技术}

针对多处理器芯片中多应用共享的特征，Intel提出了Resource Director Technology方案来解决多应用资源共享冲突问题。
如图\ref{fig:intel-rdt-overview}所示，该方案的硬件基础是资源监控与资源分配控制机制，
同时提供基于"监控->策略->控制"组合的闭环方式来实现应用感知的资源管理框架。
其中资源监控提高了资源使用情况的能见度，使得资源利用率可以被跟踪，同时可以侦测到应用性能随资源的变化，
为上层的资源调度提供数据基础；
而硬件支持的资源分配控制机制，使得上层软件可以控制对硬件共享资源的使用。

% Intel RDT Overview
\begin{figure}[H]
  \centering
  \includegraphics{x86eval/intel-rdt-overview}
  \caption[Intel Resource Director Technology (RDT) 技术示意图]{
    Intel Resource Director Technology (RDT)技术：硬件提供资源监控与分配功能，
    软件负责对资源使用进行调度，实现资源按需求动态分配。}
  \label{fig:intel-rdt-overview}
\end{figure}

目前Intel已经将RDT方案应用到共享末级缓存和内存控制器中，通过CMT/MBM技术实现对缓存容量
以及内存带宽的监控；通过CAT技术实现对缓存容量划分的支持。

CMT和MBM技术允许操作系统或Hypervisor/VMM监控在其上运行的应用对共享缓存与内存带宽
的使用情况，图\ref{fig:intel-cmt-flow}是该技术的流程示意图。
操作系统或VMM首先为执行实体（如线程、进程或虚拟机）分配资源编号RMID，后续的监控结果都将
以RMID的形式进行汇报。在OS/VMM执行调度并进行上下文切换时，将被调度实体的的RMID写入到目标
处理器核对应的寄存器中。OS/VMM可以随时通过RMID查询各个执行实体的资源使用情况，如共享缓存
占用或访存带宽等信息。
 
% Intel CMT Flow
\begin{figure}[H]
  \centering
  \includegraphics{x86eval/intel-cmt-flow}
  \caption[Intel Cache Monitor Technology (CMT) 技术流程]{Intel CMT技术流程：
   （1）为线程、应用或虚拟机等执行实体分配资源编号RMID；（2）将包含RMID的PQR寄存器
   保存在线程TCB或虚拟机VCPU中，并在执行上下文切换时写入到处理器核对应的物理寄存器中；
   （3）根据RMID使用MSRs寄存器获取共享资源使用情况。}
%Threads, applications, VMs or any combination can be associated with an RMID, enabling very flexible monitoring. As an example, all threads in a VM could be given the same RMID for simple per-VM monitoring. (2) The PQR register (containing an RMID) stored as part of a thread or VCPU state, which is written onto the thread-specific registers when a software thread is scheduled on a hardware thread for execution. (3) After a period of time (as defined by the software) the occupancy data for a given RMID can be read back through a pair of keyhole MSRs which provide the ability to input an RMID and Event ID (EvtID) in a selection MSR, and the hardware retrieves and returns the occupancy in the data MSR.}
  \label{fig:intel-cmt-flow}
\end{figure}

CAT技术为OS/VMM提供了控制末级共享缓存容量的功能，如图\ref{fig:intel-cat-flow}所示，
当该功能被开启后，应用将只能使用分配给它的Cache容量，实现路划分。
路划分策略是以COS为粒度进行指定，OS/VMM首先为某一COS制定路划分策略，并将该COS关联到使用
该策略的执行实体中，并在上下文划分时将被调度实体的COS写入到处理器核对应的寄存器中，
共享缓存根据COS对应的策略来进行缓存替换操作。

\begin{figure}[H]
  \centering
  \includegraphics[height=8cm]{x86eval/intel-cat-flow}
  \caption[Intel Cache Allocation Technology (CAT) 技术流程]{Intel CAT技术流程}
  \label{fig:intel-cat-flow}
\end{figure}


\section{标签与标签源}


\subsection{标签与隔离粒度}

如何划分地址空间：虚拟机、容器、进程、etc...

\subsection{标签寄存器}


\section{标签传播}

标签需要伴随请求在整个生命周期中传播，
这些不同类型的请求需要在各种协议类型的总线上传播（如QPI/HT、AXI、PCI-E等），
其间需要进行多次协议转换，同时会多次穿过各种Cache与Buffer，
本节首先讨论如何扩展现有的总线架构，使其能够传递标签信息，并讨论如何在保证在传播过程中请求与标签始终匹配。
%计算机中的请求主要分为三类：处理器发出的读写请求、I/O设备发出的DMA请求、中断请求。

\subsection{总线与协议转换}

不同总线上如何实现地址空间

\subsection{多阶段写回请求}

为了提高性能，计算机数据通路中采用写回（writeback）机制的写操作通常会被拆分成多个阶段。
以共享末级缓存为例，写请求的第一阶段只是将数据写入到缓存中，并将其所属的数据块标记为脏块，
只有当缓存缺失发生且该数据块被选择为替换备选时，数据才会被真正写回到内存。
如果仅使用末级缓存所接收到请求的应用标签作为写回请求的标签，可能会产生标签错误：
引起写回操作数据所属的应用与被写回的数据所属的应用不一致。
为了防止这种情况的发生，需要在共享末级缓存中，
为所有缓存的数据块额外记录其所属应用的标签Owner-DSid，
在第一阶段数据被写入缓存时，将写请求中包含的DSid记录为其Owner-DSid；
当写回操作发生时，使用其Owner-DSid作为传递到下一级的标签。
其它与共享末级缓存行为类似的具有写回机制的部件，都需要应用以上的改，
才能保证请求在通过该部件后保证应用标签的正确性。

\subsection{中断}


\section{标签应用}

\subsection{通过标签实现内存地址空间隔离}

地址映射位置，与EPT技术对比

Cache一致性场景下如何实现标签化地址空间

处理器核共享场景（0.1个处理器核）

\subsection{通过标签实现I/O地址空间隔离}

SR-IOV or pseudoMR-IOV




\if 0

\section{体系结构的可行性}

在X86或ARM上实现该架构

%如何在一个ARM中实现PARD功能

如上节所述，PARD并不是一个完全全新的体系结构，而是对现有体系结构的扩展。
为了说明如何将PARD扩展到一个现有的体系结构中，本节首先构建了一台虚拟计算机，
我们将其命名为XXXX，如图\ref{fig:XXX-computer}所示。
XXX包含两路4核的处理器，每个处理器核拥有独立的一级缓存L1-I和L1-D，
每个Socket的四个处理器核共享一个16路2MB的二级缓存；
两个Socket拥有自己的内存控制器，同时使用MESI目录协议实现NUMA内存访问；
XXX的I/O子系统包含SATA控制器和两个以太网卡，通过IOH芯片与两个处理器相连。
XXX的结构可以很好的匹配到目前流行体系结构中（如X86或ARM）。

\subsection{Computer as a Network => PARD}

\subsection{像SDN一样集中式的管理计算机}

\section{PARD与SDN}

在第\ref{chap:intro}章中我们提出
同时，计算机内部也可以被看做一个网络，如图\ref{fig:computer-as-a-network}所示，
CPU核、共享缓存、内存控制器、I/O设备等可以被看做是网络节点；除了处理请求以外，
这些“网络节点”与网络中的路由器/交换机具有相似的请求转发功能；
而它们之间也通过包进行通信，如：片内通信使用NoC包，片间通信的QPI/HT包，
以及I/O部分使用的PCI-E包。
将网络领域的区分化服务和软件定义网络的思想应用到计算机内部的网络，
用以解决数据中心当前面临的资源利用率与应用服务质量矛盾，是本文的主要研究思路与动机。

与在网络中部署SDN相比，在计算机体系结构“网络”中部署SDN会面临以下三个挑战：

首先，在整个网络栈中EndPoint是唯一的请求来源，因此SDN可以很容易的将标签机制实现在网络栈中；
与之相对的，在计算机体系结构中存在大量的硬件部件都能够发送请求，而这些请求类型又不尽相同，
因此在这样的环境下如何为请求打上标签是一个很大的挑战。

其次，在网络中所有的交换机都执行相同的存储/转发（store-and-forward）操作，但在计算机内部
不同的部件都有不同的功能，而不只是简单的存储/转发，如何为这些不同类型的部件（如末级缓存控制器、
内存控制器、I/O设备等）设计统一的控制面结构是另一个挑战。

最后，在网络交换机中已经包含了一个firmware固件用于访问和配置交换机的控制面，但计算机中却缺少
这样的firmware。现有的IPMI只被用来做有限的监控与管理功能，如对温度、风扇转速和电源控制。
因此，需要在计算机内部提供一种这样的部件实现与其它众多的控制面的通信与管理，并提供一个灵活的
编程接口对这些控制面进行操作。

\fi
