
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
\secretcontent{绝密}

\ctitle{面向数据中心应用服务质量保障的资源管理可编程\\体系结构研究}
\makeatletter
\makeatother
\cdegree{工学博士}
\cdepartment[计算所]{中国科学院计算技术研究所}
\cmajor{计算机系统结构}
\cauthor{马久跃} 
\csupervisor{孙凝晖\hspace{1em}研究员}
\csupervisorplace{中国科学院计算技术研究所}

% 日期自动生成，如果你要自己写就改这个cdate
% \cdate{\CJKdigits{\the\year}年\CJKnumber{\the\month}月}

\etitle{Programmable Architecture for Quality-of-Service in Datacenters }
\edegree{Doctor of Philosophy}
\eauthor{Ma Jiuyue}
\edepartment{Institute of Computing Technology, Chinese Academy of Sciences}
\emajor{Computer Architecture}
\esupervisor{Sun Ninghui}

% 这个日期也会自动生成，你要改么？
% \edate{December, 2005}

% 中英文摘要和关键字
\begin{cabstract}
  数据中心环境下资源利用率与应用服务质量是重要的运行维护指标，但两者存在矛盾。目
  前数据中心一般通过多应用混合提高资源利用率，但会引起干扰降低应用服务质量，进而
  影响企业关键业务收入。因此如何在数据中心多应用共享资源环境下保障应用服务质量已
  成为提高数据中心资源利用率的核心技术。软件定义网络（SDN）已能对数据中心互连网络
  进行资源与性能隔离，但服务器节点内部依然存在严重的应用相互干扰。因此，本课题提
  出一种从硬件上支持资源容量隔离与性能隔离的低开销、软件可编程的资源可编程体系结
  构PARD（Programmable Architecture for Resourcing on-Demand），其核心思想是通过
  对计算机内部共享资源控制器增加可编程的控制面，使控制器根据控制面的规则来监控与
  转发数据通路上的数据请求包，从而实现对资源的容量隔离与性能隔离。通过硬件支持资
  源隔离，PARD还能消除软件虚拟化的开销，支持多操作系统同时运行。本课题目标通过基
  于模拟器的原型系统来验证PARD技术的可行性与实际效果。

  本文的创新点主要有：
  \begin{itemize}
    \item 用例子来解释模板的使用方法；
    \item 用废话来填充无关紧要的部分；
    \item 一边学习摸索一边编写新代码。
  \end{itemize}

  关键词是为了文献标引工作、用以表示全文主要内容信息的单词或术语。关键词不超过 5
  个，每个关键词中间用分号分隔。（模板作者注：关键词分隔符不用考虑，模板会自动处
  理。英文关键词同理。）
\end{cabstract}

\ckeywords{数据中心, 服务质量}

\begin{eabstract} 
  Contemporary data centers confront with challenges in managing the trade-offs
  between resource utilization and applications’ quality of services (QoS).

  Co-locate multiple workloads into a single server is the most straightforward
  approach to achieve high utilization. In this manner, Google’s batch-workload 
  data centers achieve 75\% CPU utilization, on average.  However, co-location
  induces contention for various shared hardware resources, as well as shared
  software resources. Such contention causes unpredictable performance variability
  that is amplified at data center scales where online services involve tens to
  hundreds of servers in processing even a single user request. Moreover, such
  performance variability occurs frequently due to the unpredictability of frequent,
  short-running workloads. A month of profiling data from a 12,000-server Google
  data center shows that more than a million jobs ran for only a few minutes.

  On the other hand, to guarantee QoS of latency-critical online services, data
  center operators or developers tend to avoid sharing by either dedicating
  resources or exaggerating reservations for online services in shared environments.
  Another Google example shows that typical online-service datacenters exhibit
  only about 30\% CPU utilization, on average, much lower than batch-workload
  data centers. In fact, without co-location industry-wide utilization is even
  lower-only between 6\% to 12\%.

  Recognizing the inherent trade-offs, researchers have delved into the contention problem over the recent years. Lots of works focused on eliminate resource contention points at various levels of software stack, such as interrupt handler, OS/hypervisor scheduler or network 

  These efforts include eliminate resource contention points at various levels of the software stack, from the interrupt handler, OS scheduler, and hypervisor scheduler, to the network stack, queuing buffers, and application locks. Despite these efforts, utilization of practical online-service data centers remains low. First, software-only approaches are limited to coarse-grained management, which results in relatively poor control accuracy. Second, sophisticated software stacks and varying application demands cause contention points to change across different scenarios. Third, finding contention points usually requires tremendous time and effort.

\end{eabstract}

\ekeywords{Datacenter, QoS}
