
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

\chapter{标签化地址空间}
\label{chap:labeladdrspace}

以虚拟化技术为代表的多租户模式成为数据中心的主流应用模式。
在这种场景下，为不同用户提供良好的隔离环境是十分重要的。
现有大量技术的提出都是为了解决虚拟机之间隔离性，如：
VPID、
VMX扩展、
扩展页表EPT、
虚拟化的I/O APIC、
PCI-E单根虚拟化SR-IOV、
缓存容量划分CAT等。
隔离分为功能隔离与性能隔离，对于功能隔离，现有技术已经很好了，
能够达到虚拟机之间不存在相互干扰；
但在性能隔离方面，片内与片外存在明显差别。
以处理器核为例，
早期的软件虚拟化技术，需要软件实现处理器核状态保存与切换，还需要处理TLB和Cache无效，
Intel/AMD都在硬件上提供了处理器核虚拟化功能，如Intel的VMX和VPID技术，
在硬件上提供了处理器核状态切换，同时通过VPID技术使得切换虚拟机时无需进行TLB与Cache的无效操作；
早期guest的缺页处理需要由hypervisor拦截并处理，EPT技术增加了一级页表，将不同的虚拟机隔离开；
降低guest在hypervisor处理缺页时的干扰；
CMT技术允许给处理器核发出的访存请求打标签，另其在共享缓存层次实现应用区分，
按应用为粒度统计缓存占用、缺失率等信息；CAT在CMT的基础上实现缓存容量划分。

%
%在片外，I/O只划分不隔离，因为它离应用太远，已经没有应用的信息，
%
%Cache能做到很好的隔离是因为处理器核将标签传递到缓存
%
%但是该标签到Cache断了，
%
%所以提出标签化地址空间，将应用标签传递到整个系统，让所有的共享硬件都能够支持隔离
%标签化地址空间的目的是将多个虚拟机真正的物理隔离开。
%

标签化地址空间是PARD体系结构的基础，通过将应用标签传递到体系结构内所有的硬件部件中，
可以让这些共享的硬件部件能够识别出不同的应用，实现应用的资源隔离与性能隔离。
通过标签化地址空间以及少量的硬件修改，PARD可以很容易的将一台计算机划分为多个相互隔离的逻辑域，
在无需虚拟化软件（如KVM、Xen、VMware等）的辅助下，即可直接将这些逻辑域作为虚拟机使用；
同时，硬件能够通过标签获得更多来自上层应用的信息，辅助其调整自身的策略，以更好的服务应用。

%很多现有的技术与PARD的标签化地址空间具有想类似的理念，如EPT、SR-IOV、CAT/CMT等，
%它们都是希望在硬件上将不同的应用隔离开来，同时传递更多的应用信息到硬件。
%本章首先对标签化地址空间及其产生背景进行介绍，并与其它类似的技术进行对比，
%之后分析实现一个支持标签化地址空间的体系结构需要考虑的主要问题，
%最后讨论如何在现有体系结构实现下扩展以支持标签化地址空间。

%本章提出了一种标签化地址空间，通过将应用标签传递到整个系统，实现系统级按应用隔离，
%用以支持多租户场景下用户之间的隔离。
本章首先介绍相关工作，包括需要隔离的硬件部件，讨论在这些部件上已有的隔离技术，并分析其不足;
之后依次讨论如何在各个硬件部件上通过标签实现隔离；
最后讨论如何将标签化地址空间扩展到整个数据中心范围。


%%%虚拟地址空间对应多进程场景，随着单节点计算能力的增强以及云计算场景发展，
%%%多租户场景成为一种趋势。
%%%
%%%虚拟机抽象的出现是应对这下趋势的一个变化，将一台物理机隔离为多个无关的虚拟机，
%%%现在虚拟机基于的硬件技术包括：内存划分技术EPT、I/O虚拟化I/O-MMU、MSI中断等。
%%%这些技术在Hypervisor的支持下协同工作，向上层展现出虚拟的隔离计算机。
%%%
%%%但这里的隔离并非真正的隔离，多个虚拟机仍然通Hypervisor实现对共享硬件资源的使用，
%%%如hypervisor需要对页表寄存器进行控制，实现地址空间的切换；
%%%通过对I/O设备的代理访问，实现虚拟I/O设备；
%%%
%%%
%%%
%%%为了适应当前多应用以及多租户的使用场景，计算机系统软件栈在不断发展。
%%%以Linux操作系统为例，其包含多级嵌套结构以适应于不同的应用场景，如图\ref{}所示。
%%%操作系统最小的调度单元是线程，多个线程运行在同一个进程的地址空间中，
%%%线程之间共享全部的软硬件资源；
%%%不同的进程具有独立的虚拟地址空间，它们之间共享除用户地址空间外的全部软硬件资源，
%%%如操作系统内核；
%%%进程包含在名字空间中，一个操作系统中可同时存在多个名字空间，
%%%它们拥有自己的PID、网络、根文件系统，名字空间只共享部分操作系统内核，硬件依然是全共享；
%%%在Linux操作系统的最顶层是虚拟机，每个虚拟机都运行自己的操作系统，
%%%在操作系统及以上层次不存在共享，虚拟机之间通过Hypervisor共享硬件资源。


\section{相关工作}

体系结构对应用区分的支持，从最早的虚拟地址空间，到虚拟化支持。
本节将从资源隔离与性能隔离两个方面介绍应用区分的相关工作。


\subsection{总线技术}

总线是连接计算内所有硬件部件的数据通路，从早期的并行总线，到现在流行的点对点总线，
不同类型的总线具有不同的结构，而计算机系统的架构很大程度上决定于其所选择的总线架构。
本节选取了三种在不同场景下常用的总线结构：
（1）片内AMBA AXI总线；
（2）系统互连QPI/HT总线；
和（3）I/O总线PCI-E，
介绍其基本架构与原理，在下一节将分析如何在这些总线上扩展以支持标签化地址空间的标签传播。

\textbf{AXI总线}　AMBA Advanced eXtensible Interface（AXI）总线是由ARM公司提出的开放式片内总线协议，
通常被用在SoC芯片内。

\textbf{QPI/HT总线} QuickPath Interconnect（QPI）和HyperTransport（HT）总线分别是由
Intel和AMD主导提出的系统互连总线，主要用于连接处理器与I/O芯片组。
两者具有类似的特性，包括：高带宽、低延迟、点对点互连和基于包的数据传输，
下文将主要以HT总线为例介绍其架构与原理。

\textbf{PCI-E总线}

AXI总线与QPI/HT总线在设计时都考虑了一致性支持，实现了基于侦听和目录的一致性协议，

I/O MMU实现地址空间划分，防止I/O设备访问越界。


\subsection{I/O中断}
% LegacyIntr => MSI
在计算机内，除数据通路外，还存在一定非常重要的通路，用于在处理器与I/O设备之间传递中断信息，
早期的计算系统中使用独立的中断线来传递中断信息，这种设计需要非常多的额外引脚，
同时由于其硬连线的特性，并不能灵活的工作。

当前的中断系统设计直接使用数据总线来传递中断信息，如PCI-E标准中为的Messag Passing Interrupt（MSI），使用对特殊地址的标准访存请求实现中断信息的传递，其基本原理如图\ref{}所示，
XXXX

MSI的出现，Intel在其芯片组中增加了I/O-APIC的设计，实现中断可重编程

随着虚拟化技术的发展，提供了I/O-APIC的虚拟化支持，能够将不同的

当前的中断架构已经很好的支持的应用区分，本章后续章节将讨论如何在现有的中断架构下，
实现标签化划分。


\subsection{资源隔离技术}
% VPID, EPT, I/O MMU, Virtualized I/O-APIC, SR-IOV

最早被隔离的资源是CPU，通过保存寄存器状态，并在进程切换时切换寄存器，实现CPU资源共享。
随着内存需求的增加，虚拟内存出现，实现用户地址空间的隔离。
虚拟化的出现，对虚拟机访存性能提出需求，体系结构也做出了相应的更改。
以Intel为例，为更好的支持虚拟化环境，其VMX技术增加了VCPU抽象，

单一地址空间 => 虚拟内存（多地址空间，保护）=> 虚拟化，tagged-TLB，VT技术，多租户
                                            => 容器技术
对计算机的使用趋势是从单一应用到多应用，

\subsection{性能隔离技术}
% CAT/CMT

\subsubsection*{Intel Resource Director Technology（RDT）技术}

针对多处理器芯片中多应用共享的特征，Intel提出了Resource Director Technology方案来解决多应用资源共享冲突问题。
如图\ref{fig:intel-rdt-overview}所示，该方案的硬件基础是资源监控与资源分配控制机制，
同时提供基于"监控->策略->控制"组合的闭环方式来实现应用感知的资源管理框架。
其中资源监控提高了资源使用情况的能见度，使得资源利用率可以被跟踪，同时可以侦测到应用性能随资源的变化，
为上层的资源调度提供数据基础；
而硬件支持的资源分配控制机制，使得上层软件可以控制对硬件共享资源的使用。

% Intel RDT Overview
\begin{figure}[H]
  \centering
  \includegraphics{x86eval/intel-rdt-overview}
  \caption[Intel Resource Director Technology (RDT) 技术示意图]{
    Intel Resource Director Technology (RDT)技术：硬件提供资源监控与分配功能，
    软件负责对资源使用进行调度，实现资源按需求动态分配。}
  \label{fig:intel-rdt-overview}
\end{figure}

目前Intel已经将RDT方案应用到共享末级缓存和内存控制器中，通过CMT/MBM技术实现对缓存容量
以及内存带宽的监控；通过CAT技术实现对缓存容量划分的支持。

CMT和MBM技术允许操作系统或Hypervisor/VMM监控在其上运行的应用对共享缓存与内存带宽
的使用情况，图\ref{fig:intel-cmt-flow}是该技术的流程示意图。
操作系统或VMM首先为执行实体（如线程、进程或虚拟机）分配资源编号RMID，后续的监控结果都将
以RMID的形式进行汇报。在OS/VMM执行调度并进行上下文切换时，将被调度实体的的RMID写入到目标
处理器核对应的寄存器中。OS/VMM可以随时通过RMID查询各个执行实体的资源使用情况，如共享缓存
占用或访存带宽等信息。
 
% Intel CMT Flow
\begin{figure}[H]
  \centering
  \includegraphics{x86eval/intel-cmt-flow}
  \caption[Intel Cache Monitor Technology (CMT) 技术流程]{Intel CMT技术流程：
   （1）为线程、应用或虚拟机等执行实体分配资源编号RMID；（2）将包含RMID的PQR寄存器
   保存在线程TCB或虚拟机VCPU中，并在执行上下文切换时写入到处理器核对应的物理寄存器中；
   （3）根据RMID使用MSRs寄存器获取共享资源使用情况。}
%Threads, applications, VMs or any combination can be associated with an RMID, enabling very flexible monitoring. As an example, all threads in a VM could be given the same RMID for simple per-VM monitoring. (2) The PQR register (containing an RMID) stored as part of a thread or VCPU state, which is written onto the thread-specific registers when a software thread is scheduled on a hardware thread for execution. (3) After a period of time (as defined by the software) the occupancy data for a given RMID can be read back through a pair of keyhole MSRs which provide the ability to input an RMID and Event ID (EvtID) in a selection MSR, and the hardware retrieves and returns the occupancy in the data MSR.}
  \label{fig:intel-cmt-flow}
\end{figure}

CAT技术为OS/VMM提供了控制末级共享缓存容量的功能，如图\ref{fig:intel-cat-flow}所示，
当该功能被开启后，应用将只能使用分配给它的Cache容量，实现路划分。
路划分策略是以COS为粒度进行指定，OS/VMM首先为某一COS制定路划分策略，并将该COS关联到使用
该策略的执行实体中，并在上下文划分时将被调度实体的COS写入到处理器核对应的寄存器中，
共享缓存根据COS对应的策略来进行缓存替换操作。

\begin{figure}[H]
  \centering
  \includegraphics[height=8cm]{x86eval/intel-cat-flow}
  \caption[Intel Cache Allocation Technology (CAT) 技术流程]{Intel CAT技术流程}
  \label{fig:intel-cat-flow}
\end{figure}



\section{标签机制}

在之前的章节中已经对PARD的标签机制进行了介绍，简单来说其核心包括两点：
一是如何为计算机中的请求\textbf{标记}上正确的标签，
二是保证标签在整个计算机系统中正确\textbf{传播}。
但在具体实现标签机制时，仍然有一些问题需要考虑，
本节主要讨论其中的标记问题，传播问题将在下一节中讨论。


\subsection{标签粒度与格式}

PARD对用户提供了逻辑域抽象，因此简单的标签方案可以直接将一个逻辑域映射到为一个标签，
以逻辑域为粒度分配系统内的硬件资源（资源划分），
并对共享硬件资源的访问进行控制（性能划分），
本文所设计的FPGA原型系统（参见第\ref{chap:impl}章）选择了该方案进行实现。
另一些应用，需要在逻辑内也实现性能划分，
即运行在同一个逻辑域的不同应用具有不同的资源访问级别，
基于容器的技术的轻量级虚拟化即属于该类型，运行在不同容器中的应用虽然具有不同的优先级，
但它们能够访问相同的资源。
为了支持这种类型的应用，本文提出了两级标签的概念，如图\ref{fig:tagging-format}所示，
将标签划分为资源域与性能域两部分，
不同资源域的应用在硬件资源上相互隔离，且具有不同的性能策略；
具有相同资源域的应用共享同一份硬件资源，
但这些共享的硬件资源使用不同的性能策略来处理来自不同性能域的应用。

\begin{figure}[tb]
  \centering
  \includegraphics[width=\textwidth]{label/tagging-format}
  \caption[标签格式]{标签格式：分为简单标签与两级标签，
    简单标签直接将逻辑域（LDom ID）映射为标签，
    两级标签将标签分为资源域（Resource ID）和性能域（Perf ID）两部分。
    只有两级标签的性能域部分可以由用户进行修改，
    简单标签或两级标签的资源域（图中阴影部分）只能通过PRM进行修改，
    以保证硬件资源分配不会出现冲突。}
  \label{fig:tagging-format}
\end{figure}

\subsection{如何为请求打标签}
在确定了标签的格式与内容后，需要将该标签打到对应的请求上。
在计算机中存在两个请求发起源，即处理器核与I/O设备的DMA引擎，
需要在这两个请求源将应用标签标记到请求上。
% 外部标签也在该节讨论

\begin{figure}[b]
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{label/tagging-core-request}
  \caption{为处理器核请求打标签}
  \label{fig:tagging-core-request}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{label/tagging-io-request}
  \caption{为I/O请求打标签}
  \label{fig:tagging-io-request}
\end{minipage}
\end{figure}

\textbf{处理器核}\quad
为实现处理器核发出的请求的标记，
需要在处理器核内增加一个标签寄存器，将当前正在使用该处理器核的应用标签保存在该寄存器中。
对于简单标签模式，该寄存器对处理器核不可见，只能通过PRM在逻辑域切换时进行修改；
对于两级标签模式，该寄存器的性能域部分对处理器可见，
可以由逻辑域内的操作系统在进行切换时进行修改，而资源域与简单模式下相同，只能由PRM进行修改。
图\ref{fig:tagging-core-request}给出了两级标签模式下，处理器核的请求标记过程：
处理器核发出的请求与标签寄存器的值进行组成，形成带有标签的请求发送到外部；
标签寄存器的资源域部分只能由PRM进行修改，性能域部分可以通过处理器核进行修改。

\textbf{I/O与中断请求}\quad
对于I/O访问，有两种访问模式，即Programmed-I/O（PIO）和Directed Memory Access（DMA)。
对于PIO访问请求，由于I/O请求已经在处理器核端进行了标记，
因此在设备端直接将请求所附带的标签返回即可，但对于DMA请求进行标签标记则需要进行额外的处理。

在介绍PARD的DMA请求标签机制前，先对DMA的工作原理进行简要回顾。
通常一个DMA请求过程可以分为三个阶段：
首先设备驱动发送一个“DMA描述符”首地址到DMA控制器，
该“DMA描述符”包含了DMA的缓存信息，例如起始地址、缓存大小、状态；
当初始化完成之后，DMA控制器加载描述符，从中获取每个DMA操作必要的信息并开始数据传输；
最后当所有数据都传输完成后，DMA控制器产生中断告诉CPU数据处理完成。

为了实现DMA请求的标签机制，PARD在每个DMA控制器中都加入一个“标签寄存器”，
如图\ref{fig:tagging-io-request}所示。
在逻辑域创建过程中，PRM对设备进行分配，将逻辑域的资源标签写入到该标签寄存器中。
对于DMA请求的三个步骤，标签寄存器相应的操作如下：
（1）初始化标签寄存器性能域并读取DMA描述符。当设备驱动向DMA引擎写如DMA描述符信息的同时，
将请求相关的标签性能域部分写入到标签寄存器中，
DMA引擎将标签寄存器中的值附加到DMA描述符读取请求中。
（2）标记数据传输请求。当设备的DMA引擎与内存控制器进行收发数据时，
从其标签寄存器中取出标签，将每个数据传输请求上都打该标签。
（3）标记中断信号。对于中断，PARD对当前的中断控制器
（Advanced Programmable Interrupt Controller, APIC）进行适当的扩充。
在APIC中增加多个中断映射表，其中的每一个都与一个应用标签进行关联。
这样当DMA引擎将要产生一个中断时，把应用标签标记到中断请求中，然后发往APIC。
APIC使用请求中的应用标签来获取相关的映射表，
根据表中的信息将中断请求转发给指定的处理器核。


\section{标签传播}

标签需要伴随请求在整个生命周期中传播，
这些不同类型的请求需要在各种协议类型的总线上传播（如QPI/HT、AXI、PCI-E等），
其间需要进行多次协议转换，同时会多次穿过各种Cache与Buffer，
本节首先讨论如何扩展现有的总线架构，使其能够传递标签信息，并讨论如何在保证在传播过程中请求与标签始终匹配。
%计算机中的请求主要分为三类：处理器发出的读写请求、I/O设备发出的DMA请求、中断请求。

\subsection{总线与协议转换}

不同总线上如何实现地址空间

\subsection{一致性协议}

基于snoop的一致性协议，在处理snoop请求时，需要检查其标签，是否与当前标签一致
基于MESI协议，需要在目录中增加标签项，

\subsection{多阶段写回请求}

为了提高性能，计算机数据通路中采用写回（writeback）机制的写操作通常会被拆分成多个阶段。
以共享末级缓存为例，写请求的第一阶段只是将数据写入到缓存中，并将其所属的数据块标记为脏块，
只有当缓存缺失发生且该数据块被选择为替换备选时，数据才会被真正写回到内存。
如果仅使用末级缓存所接收到请求的应用标签作为写回请求的标签，可能会产生标签错误：
引起写回操作数据所属的应用与被写回的数据所属的应用不一致。
为了防止这种情况的发生，需要在共享末级缓存中，
为所有缓存的数据块额外记录其所属应用的标签Owner-DSid，
在第一阶段数据被写入缓存时，将写请求中包含的DSid记录为其Owner-DSid；
当写回操作发生时，使用其Owner-DSid作为传递到下一级的标签。
其它与共享末级缓存行为类似的具有写回机制的部件，都需要应用以上的改，
才能保证请求在通过该部件后保证应用标签的正确性。

\subsection{中断}


\section{标签应用}

\subsection{通过标签实现内存地址空间隔离}

地址映射位置，与EPT技术对比

Cache一致性场景下如何实现标签化地址空间

处理器核共享场景（0.1个处理器核）

\subsection{通过标签实现I/O地址空间隔离}

SR-IOV or pseudoMR-IOV




\if 0

\section{体系结构的可行性}

在X86或ARM上实现该架构

%如何在一个ARM中实现PARD功能

如上节所述，PARD并不是一个完全全新的体系结构，而是对现有体系结构的扩展。
为了说明如何将PARD扩展到一个现有的体系结构中，本节首先构建了一台虚拟计算机，
我们将其命名为XXXX，如图\ref{fig:XXX-computer}所示。
XXX包含两路4核的处理器，每个处理器核拥有独立的一级缓存L1-I和L1-D，
每个Socket的四个处理器核共享一个16路2MB的二级缓存；
两个Socket拥有自己的内存控制器，同时使用MESI目录协议实现NUMA内存访问；
XXX的I/O子系统包含SATA控制器和两个以太网卡，通过IOH芯片与两个处理器相连。
XXX的结构可以很好的匹配到目前流行体系结构中（如X86或ARM）。

\subsection{Computer as a Network => PARD}

\subsection{像SDN一样集中式的管理计算机}

\section{PARD与SDN}

在第\ref{chap:intro}章中我们提出
同时，计算机内部也可以被看做一个网络，如图\ref{fig:computer-as-a-network}所示，
CPU核、共享缓存、内存控制器、I/O设备等可以被看做是网络节点；除了处理请求以外，
这些“网络节点”与网络中的路由器/交换机具有相似的请求转发功能；
而它们之间也通过包进行通信，如：片内通信使用NoC包，片间通信的QPI/HT包，
以及I/O部分使用的PCI-E包。
将网络领域的区分化服务和软件定义网络的思想应用到计算机内部的网络，
用以解决数据中心当前面临的资源利用率与应用服务质量矛盾，是本文的主要研究思路与动机。

与在网络中部署SDN相比，在计算机体系结构“网络”中部署SDN会面临以下三个挑战：

首先，在整个网络栈中EndPoint是唯一的请求来源，因此SDN可以很容易的将标签机制实现在网络栈中；
与之相对的，在计算机体系结构中存在大量的硬件部件都能够发送请求，而这些请求类型又不尽相同，
因此在这样的环境下如何为请求打上标签是一个很大的挑战。

其次，在网络中所有的交换机都执行相同的存储/转发（store-and-forward）操作，但在计算机内部
不同的部件都有不同的功能，而不只是简单的存储/转发，如何为这些不同类型的部件（如末级缓存控制器、
内存控制器、I/O设备等）设计统一的控制面结构是另一个挑战。

最后，在网络交换机中已经包含了一个firmware固件用于访问和配置交换机的控制面，但计算机中却缺少
这样的firmware。现有的IPMI只被用来做有限的监控与管理功能，如对温度、风扇转速和电源控制。
因此，需要在计算机内部提供一种这样的部件实现与其它众多的控制面的通信与管理，并提供一个灵活的
编程接口对这些控制面进行操作。

\fi
